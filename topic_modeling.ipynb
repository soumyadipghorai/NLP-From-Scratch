{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL2R6Lmof7Nl",
        "outputId": "fc53775c-da27-48e6-8d54-9e237997cf1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.126*\"labour\" + 0.107*\"elect\" + 0.107*\"blair\" + 0.101*\"brown\" + '\n",
            "  '0.100*\"game\" + 0.100*\"tax\" + 0.094*\"parti\" + 0.094*\"film\" + 0.090*\"tori\" + '\n",
            "  '0.087*\"peopl\"'),\n",
            " (1,\n",
            "  '0.275*\"labour\" + 0.228*\"blair\" + 0.226*\"elect\" + 0.205*\"brown\" + '\n",
            "  '0.195*\"tori\" + 0.193*\"tax\" + 0.188*\"parti\" + -0.158*\"film\" + -0.139*\"game\" '\n",
            "  '+ 0.131*\"howard\"'),\n",
            " (2,\n",
            "  '0.178*\"mobil\" + -0.167*\"film\" + -0.140*\"award\" + -0.136*\"best\" + '\n",
            "  '0.134*\"phone\" + 0.119*\"growth\" + -0.114*\"win\" + -0.112*\"england\" + '\n",
            "  '0.102*\"bn\" + 0.097*\"bank\"'),\n",
            " (3,\n",
            "  '-0.355*\"film\" + -0.209*\"award\" + 0.168*\"england\" + -0.148*\"best\" + '\n",
            "  '-0.139*\"oscar\" + -0.124*\"nomin\" + -0.115*\"music\" + -0.114*\"actor\" + '\n",
            "  '0.103*\"game\" + -0.102*\"star\"'),\n",
            " (4,\n",
            "  '0.223*\"mobil\" + -0.184*\"film\" + 0.178*\"phone\" + -0.147*\"growth\" + '\n",
            "  '-0.143*\"economi\" + -0.142*\"bn\" + -0.137*\"dollar\" + -0.137*\"bank\" + '\n",
            "  '-0.130*\"rate\" + 0.117*\"game\"')]\n"
          ]
        }
      ],
      "source": [
        "# find the number of topics in the given corpus\n",
        "\n",
        "import os\n",
        "import gensim\n",
        "from gensim.models import LsiModel\n",
        "from gensim import models\n",
        "from gensim import corpora\n",
        "from gensim.utils import lemmatize\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.parsing.preprocessing import remove_stopwords, stem_text\n",
        "from gensim.parsing.preprocessing import strip_numeric, strip_short,strip_multiple_whitespaces,strip_non_alphanum,strip_punctuation,strip_tags,preprocess_string\n",
        "import pandas as pd\n",
        "from gensim import similarities\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "\n",
        "#read the data\n",
        "corpus_dir = 'https://raw.githubusercontent.com/Ramaseshanr/anlp/master/corpus/bbc-text.csv'\n",
        "df_corpus = pd.read_csv(corpus_dir,names=['category', 'text'])\n",
        "corpus = df_corpus['text'].values.tolist()\n",
        "corpus = corpus[1:]\n",
        "my_filter = [\n",
        "    lambda x: x.lower(), strip_tags, strip_punctuation,\n",
        "    strip_multiple_whitespaces, strip_numeric,\n",
        "    remove_stopwords, strip_short, stem_text\n",
        "]\n",
        "\n",
        "\n",
        "def preprocessing(corpus):\n",
        "\n",
        "    for document in corpus:\n",
        "        doc = strip_numeric(document)\n",
        "        doc = remove_stopwords(doc)\n",
        "        doc = strip_short(doc,3)\n",
        "        doc = stem_text(doc)\n",
        "        doc = strip_punctuation(doc)\n",
        "        strip_tags(doc)\n",
        "        yield gensim.utils.tokenize(doc, lower=True)\n",
        "\n",
        "\n",
        "texts = preprocessing(corpus)\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "dictionary.filter_extremes(no_below=1, keep_n=25000)\n",
        "\n",
        "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in preprocessing(corpus)]\n",
        "tfidf = models.TfidfModel(doc_term_matrix)\n",
        "corpus_tfidf = tfidf[doc_term_matrix]\n",
        "\n",
        "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary)  # initialize an LSI transformation\n",
        "pprint(lsi.print_topics(num_topics=5, num_words=10)) #num words in each topic\n",
        "\n",
        "\n",
        "# lot of words in the corpus has very less freq that's why we are unable to find the pattern\n",
        "# which results into large value of sigma(n).\n",
        "# the smaller the sigma(n) the better the result\n",
        "\n",
        "# also there are a lot of words common in the both the topics like sports and film"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIswEv-egDtW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}